{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/__init__.py:1467: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import copy\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.applications.mobilenet import MobileNet\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Lambda, Activation, concatenate, LSTM\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow.keras.optimizers\n",
    "from keras.utils import np_utils\n",
    "#from tensorflow.keras import backend as K\n",
    "#K.tensorflow_backend._get_available_gpus()\n",
    "tf.test.is_gpu_available(cuda_only=True)\n",
    "\n",
    "try:\n",
    "    os.mkdir('results/3sampleclassification/')\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "height,width=(540,540)\n",
    "\n",
    "\n",
    "def getvideoname(name):\n",
    "    return '_'.join(name.split('/')[-1].split('_')[:3])\n",
    "\n",
    "def getframe(name):\n",
    "    return name.split('/')[-1].split('_')[-1].split('.')[0]\n",
    "\n",
    "def getlabel(name):\n",
    "    if len(name.split('/')[-1].split('_'))>=5:\n",
    "        try:\n",
    "            return [float(x) for x in name.split('/')[-1].split('_')[3:-1]]\n",
    "        except:\n",
    "            return [0,0,0]\n",
    "    else:\n",
    "        return [0,0,0]\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    dataset_path = 'trainingdata/'\n",
    "    frames=glob.glob(dataset_path+'**/*frames_face/*.*.jpg')\n",
    "    indexes=[x for x in range(len(frames))]\n",
    "    datadict=defaultdict(list)\n",
    "    labelsdict={}\n",
    "    for frame in frames:\n",
    "        datadict[getvideoname(frame)].append(frame)\n",
    "    for key,data in datadict.items():\n",
    "        #print(key)\n",
    "        datadict[key] = sorted(data, key=lambda x:int(getframe(x)))\n",
    "        labelsdict[key]=[getlabel(x) for x in data]\n",
    "    #random.shuffle(indexes)\n",
    "    return indexes,datadict,labelsdict\n",
    "\n",
    "def find_index(index,data,labels):\n",
    "    for key in data.keys():\n",
    "        if index>=len(data[key]) - 2:\n",
    "            index-=len(data[key])\n",
    "            continue\n",
    "        return data[key][index],labels[key][index]\n",
    "\n",
    "\n",
    "def get_img(index, indexes, X, Y):\n",
    "    x,y=find_index(index,X,Y)\n",
    "    if Y==[0,0,0]:\n",
    "        x,y=find_index(random.randint(len(indexes),X,Y))\n",
    "    #print(x)\n",
    "    x=Image.open(x)\n",
    "    x.convert('RGB')\n",
    "    x=np.array(x)\n",
    "    #x.resize((224,224,3))\n",
    "    return x,y\n",
    "\n",
    "def data_generator(X,Y,indexes,batch_size=12):\n",
    "    while True:\n",
    "        idxs=np.random.permutation(len(indexes))\n",
    "        p0,p1,p2, q = [],[],[],[]\n",
    "        for i in range(len(X) - 1):\n",
    "            x_2, y_2 = get_img(idxs[i]+2, indexes, X, Y)\n",
    "            # while y_1==[0,0,0]:\n",
    "            #     tempindex=random.randint(len(indexes)-1)\n",
    "            #     x_1, y_1 = get_img(temp+1, indexes, X, Y)\n",
    "            # x,y = get_img(temp, indexes, X, Y)\n",
    "            x_1, y_1 = get_img(idxs[i]+1, indexes, X, Y)\n",
    "            x,y = get_img(idxs[i], indexes, X, Y)\n",
    "            #while y_2==[0,0,0]:\n",
    "            #    tempindex=random.randint(len(indexes)-2)\n",
    "            #    x_2, y_2 = get_img(tempindex+2, indexes, X, Y)\n",
    "            #    x_1, y_1 = get_img(tempindex+1, indexes, X, Y)\n",
    "            #    x,y = get_img(tempindex+1, indexes, X, Y)\n",
    "            \n",
    "\n",
    "            p0.append(x)\n",
    "            p1.append(x_1)\n",
    "            p2.append(x_2)\n",
    "            #print(str(y_2[0])+' '+str(converttolabel(y_2[0])))\n",
    "            q.append(converttolabel(y_2[1]))\n",
    "            if len(q)==batch_size:\n",
    "                yield {'input1': np.array(p0), 'input2': np.array(p1),'input3':np.array(p2)}, np.array(q)\n",
    "                p0, p1,p2, q=[],[], [],[]\n",
    "        if p0:\n",
    "            yield {'input1': np.array(p0), 'input2': np.array(p1),'input3':np.array(p2)}, np.array(q)\n",
    "            p0, p1,p2, q=[],[],[], []\n",
    "\n",
    "def custom_stack_layer(tensor):\n",
    "    return tf.keras.backend.stack(\n",
    "        tensor, axis=1\n",
    "    )\n",
    "\n",
    "def converttolabel(num):\n",
    "    #return int((num+1.1912906024096384)*10)\n",
    "    return int((num+0.24397519277108434)*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38341\n",
      "0.9413920541894304\n",
      "-0.24397519277108434\n",
      "5.768446506024094\n"
     ]
    }
   ],
   "source": [
    "_,_l,labeldict=load_data()\n",
    "#print(labeldict)\n",
    "nums,avg,minnum,maxnum=0,0,10000,-10000\n",
    "for x,labels in labeldict.items():\n",
    "    if min([i[1] for i in labels])<minnum: minnum=min([i[1] for i in labels])\n",
    "    if max([i[1] for i in labels])>maxnum: maxnum=max([i[1] for i in labels])  \n",
    "    for label in labels:\n",
    "        \n",
    "        if label==[0,0,0]:\n",
    "            continue\n",
    "        else:\n",
    "            nums+=1\n",
    "            avg+=label[1]\n",
    "avg/=nums\n",
    "print(nums)\n",
    "print(avg)\n",
    "print(minnum)\n",
    "print(maxnum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras_applications/mobilenet.py:207: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "  warnings.warn('`input_shape` is undefined or non-square, '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "799/800 [============================>.] - ETA: 0s - loss: 2.3483 - acc: 0.4371Epoch 1/50\n",
      "800/800 [==============================] - 537s 672ms/step - loss: 2.3478 - acc: 0.4370 - val_loss: 2.4091 - val_acc: 0.3551\n",
      "Epoch 2/50\n",
      "799/800 [============================>.] - ETA: 0s - loss: 1.6879 - acc: 0.5513Epoch 1/50\n",
      "800/800 [==============================] - 519s 648ms/step - loss: 1.6875 - acc: 0.5511 - val_loss: 2.0324 - val_acc: 0.4577\n",
      "Epoch 3/50\n",
      "799/800 [============================>.] - ETA: 0s - loss: 1.6574 - acc: 0.5289Epoch 1/50\n",
      "800/800 [==============================] - 520s 650ms/step - loss: 1.6570 - acc: 0.5288 - val_loss: 1.9814 - val_acc: 0.4623\n",
      "Epoch 4/50\n",
      "799/800 [============================>.] - ETA: 0s - loss: 1.6275 - acc: 0.5253Epoch 1/50\n",
      "800/800 [==============================] - 518s 648ms/step - loss: 1.6279 - acc: 0.5253 - val_loss: 1.9956 - val_acc: 0.4259\n",
      "Epoch 5/50\n",
      "799/800 [============================>.] - ETA: 0s - loss: 1.5485 - acc: 0.5555Epoch 1/50\n",
      "800/800 [==============================] - 522s 652ms/step - loss: 1.5477 - acc: 0.5558 - val_loss: 1.9336 - val_acc: 0.4373\n",
      "Epoch 6/50\n",
      "799/800 [============================>.] - ETA: 0s - loss: 1.4669 - acc: 0.5532Epoch 1/50\n",
      "800/800 [==============================] - 516s 645ms/step - loss: 1.4679 - acc: 0.5531 - val_loss: 1.7103 - val_acc: 0.4392\n",
      "Epoch 7/50\n",
      "799/800 [============================>.] - ETA: 0s - loss: 1.4202 - acc: 0.5619Epoch 1/50\n",
      "800/800 [==============================] - 516s 645ms/step - loss: 1.4208 - acc: 0.5620 - val_loss: 1.7207 - val_acc: 0.4415\n",
      "Epoch 8/50\n",
      "799/800 [============================>.] - ETA: 0s - loss: 1.4022 - acc: 0.5569Epoch 1/50\n",
      "800/800 [==============================] - 511s 638ms/step - loss: 1.4018 - acc: 0.5570 - val_loss: 1.6839 - val_acc: 0.4620\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'results/3sampleclassification/hist.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-3331535a2f5f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0mhist_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"results/3sampleclassification/hist.csv\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m     \u001b[0mhist_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'results/3sampleclassification/hist.csv'"
     ]
    }
   ],
   "source": [
    "indexes,data,labels=load_data()\n",
    "randomprints=open('random.txt','w')\n",
    "#randomprints.write(len(indexes))\n",
    "\n",
    "input_image=Input(shape=(height,width,3), name='input1')\n",
    "input_image2=Input(shape=(height,width,3), name='input2')\n",
    "input_image3=Input(shape=(height,width,3), name='input3')\n",
    "randomprints.write(str(input_image.shape))\n",
    "randomprints.flush()\n",
    "mobilenet = MobileNet(include_top=False,pooling='avg')\n",
    "x1 = mobilenet(input_image)\n",
    "x2 = mobilenet(input_image2)\n",
    "x3= mobilenet(input_image3)\n",
    "x = Lambda(custom_stack_layer, name=\"lambda_layer\")([x1,x2,x3])\n",
    "\n",
    "lstm = LSTM(\n",
    "    units=256, return_sequences=False, dropout=0.2\n",
    ")(x)\n",
    "\n",
    "out = Dense(60, activation='relu', trainable=False, name='out')(lstm)\n",
    "\n",
    "predict=Activation(activation='softmax')(out)\n",
    "\n",
    "model=Model(inputs=[input_image, input_image2,input_image3], outputs=predict)\n",
    "model.compile(optimizer='adam',loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "train_indexes=indexes[:int(0.7*len(indexes))]\n",
    "validation_indexes=indexes[int(0.7*len(indexes)):]\n",
    "\n",
    "earlystop = tf.keras.callbacks.EarlyStopping( monitor = 'val_acc', patience=5 )\n",
    "reducelr = tf.keras.callbacks.ReduceLROnPlateau( monitor = 'val_acc', patience=2, factor=0.2, min_lr=0.0001 )\n",
    "\n",
    "history=model.fit_generator(data_generator(data,labels,train_indexes),\n",
    "                            steps_per_epoch=800,\n",
    "                            epochs=50,\n",
    "                            validation_data=data_generator(data,labels,validation_indexes),\n",
    "                            validation_steps=385,\n",
    "                            callbacks=[earlystop,reducelr])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "hist_df=pd.DataFrame(history.history)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"histlabel1.csv\",mode='w') as f:\n",
    "    hist_df.to_json(f)\n",
    "\n",
    "\n",
    "model_json=model.to_json()\n",
    "with open(\"MobileNetTransferlabel1.json\",'w') as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "model.save_weights('MobileNetTransferlabel1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_p36] *",
   "language": "python",
   "name": "conda-env-tensorflow_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
